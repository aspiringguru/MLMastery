{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification \n",
    "from pandas import read_csv \n",
    "from sklearn.model_selection  import KFold \n",
    "from sklearn.model_selection import cross_val_score \n",
    "#\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "#\n",
    "from sklearn.svm import SVC \n",
    "import pandas as pd\n",
    "print (\"libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "filename = './data/pima-indians-diabetes.data.csv' \n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names) \n",
    "print (dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = dataframe.values \n",
    "X = array[:,0:8] \n",
    "Y = array[:,8] \n",
    "\n",
    "seed = 7 \n",
    "n_splits = 10\n",
    "#make 10 kfolds, use random seed = 7\n",
    "#use DecisionTreeClassifier with 100 trees\n",
    "#cross validate results and calc mean.\n",
    "kfold = KFold(n_splits=n_splits, random_state=seed) \n",
    "\n",
    "masterResults = []\n",
    "estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.76951469583\n",
      "max: 0.857142857143\n",
      "min: 0.701298701299\n",
      "std dev: 0.0484105192457\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() \n",
    "results = cross_val_score(model, X, Y, cv=kfold) \n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(('LogisticRegression', results))\n",
    "estimators.append(('LogisticRegression',model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.770745044429\n",
      "max: 0.857142857143\n",
      "min: 0.636363636364\n",
      "std dev: 0.073867901366\n"
     ]
    }
   ],
   "source": [
    "cart = DecisionTreeClassifier() \n",
    "num_trees = 100 \n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score(model, X, Y, cv=kfold) \n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(('BaggingClassifier', results))\n",
    "estimators.append(('BaggingClassifier',cart))#nb: this one is different to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.76035543404\n",
      "max: 0.87012987013\n",
      "min: 0.649350649351\n",
      "std dev: 0.0726626172806\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100 \n",
    "max_features = 3 \n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(['RandomForestClassifier', results])\n",
    "estimators.append(('RandomForestClassifier',model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.764217361586\n",
      "max: 0.857142857143\n",
      "min: 0.675324675325\n",
      "std dev: 0.0623214808086\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100 \n",
    "max_features = 7 \n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(('ExtraTreesClassifier', results))\n",
    "estimators.append(('ExtraTreesClassifier',model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.76045796309\n",
      "max: 0.831168831169\n",
      "min: 0.675324675325\n",
      "std dev: 0.0544377762728\n"
     ]
    }
   ],
   "source": [
    "num_trees = 30 \n",
    "seed=7 \n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score(model, X, Y, cv=kfold) \n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(('AdaBoostClassifier', results))\n",
    "estimators.append(('AdaBoostClassifier',model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10,)\n",
      "mean: 0.766900205058\n",
      "max: 0.831168831169\n",
      "min: 0.636363636364\n",
      "std dev: 0.0565956872779\n"
     ]
    }
   ],
   "source": [
    "seed = 7 \n",
    "num_trees = 100 \n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print (type(results), results.shape)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n",
    "masterResults.append(('GradientBoostingClassifier', results))\n",
    "estimators.append(('GradientBoostingClassifier',model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            LogisticRegression\n",
       "1             BaggingClassifier\n",
       "2        RandomForestClassifier\n",
       "3          ExtraTreesClassifier\n",
       "4            AdaBoostClassifier\n",
       "5    GradientBoostingClassifier\n",
       "Name: Classifier, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(masterResults, columns=['Classifier','results'])\n",
    "df['Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model \n",
    "ensemble = VotingClassifier(estimators) \n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[ 0.67532468  0.83116883  0.74025974  0.64935065  0.81818182  0.81818182\n",
      "  0.81818182  0.83116883  0.72368421  0.78947368]\n",
      "mean: 0.769497607656\n",
      "max: 0.831168831169\n",
      "min: 0.649350649351\n",
      "std dev: 0.0643224170391\n"
     ]
    }
   ],
   "source": [
    "print (results.shape)\n",
    "print (results)\n",
    "print (\"mean:\", results.mean())  \n",
    "print (\"max:\", max(results))\n",
    "print (\"min:\", min(results))\n",
    "print (\"std dev:\", results.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
